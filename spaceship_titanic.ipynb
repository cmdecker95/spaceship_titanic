{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SPACESHIP TITANIC\n",
    "First, I will import the libraries and data needed for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the training dataset (saving the test dataset for final submission)\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Count and enumerate unique categorical values\n",
    "for col in df_train.columns:\n",
    "    column = df_train[col]\n",
    "    if column.dtype in ('bool', 'object'):\n",
    "        uniques = column.unique()\n",
    "        print(f\"{col} ({len(uniques)})\")\n",
    "        print(f\"{uniques}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Show feature correlation heatmap\n",
    "plt.figure(figsize=(15, 5), dpi=150)\n",
    "sns.heatmap(df_train.corr(), annot=True, cmap='cubehelix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PLAN OF ATTACK\n",
    "For this analysis, I want to see if there is a performance difference between the predictions on the normal feature space and predictions on a principal component latent space.\n",
    "<br>\n",
    "##### **Principal Component Analysis**\n",
    "There are some binary categorical features, and there are some nominal categorical features. If I use one-hot encoding, then I'll significantly expand the dimensionality, especially with features like `Name` and `Cabin`. However, I wonder if running PCA for dimensionality reduction will prove insightful, possibly by finding otherwise-hidden patterns or clusters in the latent space.\n",
    "<br>\n",
    "##### **Artificial Neural Network**\n",
    "I'll use Keras to build a low-depth neural net, tweaking the parameters (architecture) and hyperparameters (Adam parameters) until the model is complex enough not to underfit to the data and regularized enough not to overfit to the data. Then, I will compare the performance of the neural net on the normal feature space and the PCA latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# FEATURE ENGINEERING: Trial 1\n",
    "I will fully expand every feature through dummy encoding, then I'll re-reduce dimensionality with PCA. Predictions will be on the latent space. I am concerned that the SVD algorithm won't be able to converge due to the very high dimensionality I'm anticipating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Extract features and labels from the training data\n",
    "X = df_train.iloc[:, :-1]\n",
    "y = df_train.iloc[:, -1:]\n",
    "print(f\"{X.shape=}\")\n",
    "print(f\"{y.shape=}\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Expand all features\n",
    "X_clean = pd.get_dummies(X)\n",
    "X_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Standardize the features about the origin\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clean)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_clean.columns)\n",
    "X_scaled[X_scaled.isna()] = 0\n",
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Binarize boolean labels\n",
    "y_clean = np.zeros(y.shape) + y.to_numpy()\n",
    "y_clean = pd.DataFrame(y_clean, dtype='uint8')\n",
    "y_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split 20% of data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_clean, test_size=0.2, shuffle=True, random_state=0)\n",
    "\n",
    "print(f\"{X_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{X_val.shape=}\")\n",
    "print(f\"{y_val.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Code below is retroactively commented out because it doesn't converge.\n",
    "\n",
    "# Decompose into principal components\n",
    "# pca = PCA(random_state=0).fit(X_train)\n",
    "# X_train_pca = pca.transform(X_train)\n",
    "#\n",
    "# pca_val = PCA(random_state=0).fit(X_val)\n",
    "# X_val_pca = pca.transform(X_val)\n",
    "#\n",
    "# print(f\"{X_train_pca.shape=}\")\n",
    "# print(f\"{X_val_pca.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# FEATURE ENGINEERING: Trial 2\n",
    "Indeed, the SVD algorithm did not converge because of the dimensionality. To prevent this issue, I'll conduct this trial the same as Trial 1, except I will be more intentional when encoding categorical data instead of just naively expanding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up the features\n",
    "\n",
    "# `Name` and `PassengerId` are unique identifiers, probably not highly correlated to the target\n",
    "X_clean = X.drop(['Name', 'PassengerId'], axis=1)\n",
    "\n",
    "# Only dummy encode features with fewer than 5 categories\n",
    "expand_cols = ['HomePlanet', 'Destination']\n",
    "onehot_cols = pd.get_dummies(X_clean[expand_cols])\n",
    "X_clean = X_clean.drop(expand_cols, axis=1)\n",
    "X_clean = X_clean.join(onehot_cols)\n",
    "\n",
    "# Binarize boolean features\n",
    "for col in ('CryoSleep', 'VIP'):\n",
    "    column = X_clean[col].copy()\n",
    "    column[column.isna()] = 0\n",
    "    column = np.zeros(column.shape) + column.to_numpy()\n",
    "    column = pd.Series(column, dtype='uint8')\n",
    "    X_clean[col] = column\n",
    "\n",
    "# Split `Cabin` encoding by splitting it into 3 features\n",
    "cabins = {'Cabin1': [],\n",
    "          'Cabin2': [],\n",
    "          'Cabin3': []}\n",
    "\n",
    "for i, row in X_clean.copy().iterrows():\n",
    "    if type(row['Cabin']) == str:\n",
    "        c1, c2, c3 = row['Cabin'].split('/')\n",
    "        cabins['Cabin1'].append(c1)\n",
    "        cabins['Cabin2'].append(c2)\n",
    "        cabins['Cabin3'].append(c3)\n",
    "\n",
    "X_clean = X_clean.drop('Cabin', axis=1)\n",
    "X_clean = X_clean.join(pd.DataFrame(cabins))\n",
    "\n",
    "expand_cabins = ['Cabin1', 'Cabin2', 'Cabin3']\n",
    "onehot_cabins = pd.get_dummies(X_clean[expand_cabins])\n",
    "X_clean = X_clean.drop(expand_cabins, axis=1)\n",
    "X_clean = X_clean.join(onehot_cabins)\n",
    "\n",
    "X_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize features between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_clean)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_clean.columns)\n",
    "X_scaled[X_scaled.isna()] = 0\n",
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split 20% of data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_clean, test_size=0.2, shuffle=True, random_state=0)\n",
    "\n",
    "print(f\"{X_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{X_val.shape=}\")\n",
    "print(f\"{y_val.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Decompose into principal components\n",
    "pca = PCA(random_state=0).fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "\n",
    "pca_val = PCA(random_state=0).fit(X_val)\n",
    "X_val_pca = pca.transform(X_val)\n",
    "\n",
    "print(f\"{X_train_pca.shape=}\")\n",
    "print(f\"{X_val_pca.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize cumulative explained variance\n",
    "exp_var = np.array([sum(pca.explained_variance_ratio_[:(i + 1)]) for i in range(pca.n_components_)])\n",
    "exp_var_val = np.array([sum(pca_val.explained_variance_ratio_[:(i + 1)]) for i in range(pca_val.n_components_)])\n",
    "\n",
    "plt.style.use('seaborn-dark')\n",
    "plt.figure(figsize=(15, 5), dpi=150)\n",
    "plt.plot(exp_var, label=\"Training Data Variance\")\n",
    "plt.plot(exp_var_val, label=\"Validation Data Variance\")\n",
    "\n",
    "plt.title(\"Cumulative explained variance within the first principal components\")\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.ylabel(\"Variance explained\")\n",
    "plt.xticks([_ for _ in np.arange(0, 2000, 100)])\n",
    "plt.yticks([_ for _ in np.arange(0, 1.1, .1)])\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot data projections\n",
    "plt.figure(figsize=(15, 15), dpi=150)\n",
    "plt.suptitle(\"First eight principal components, colored by target label\")\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "sns.scatterplot(x=X_train_pca @ pca.components_[0],\n",
    "                y=X_train_pca @ pca.components_[1],\n",
    "                hue=y_train[0])\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.xlabel(\"PC3\")\n",
    "plt.ylabel(\"PC4\")\n",
    "sns.scatterplot(x=X_train_pca @ pca.components_[2],\n",
    "                y=X_train_pca @ pca.components_[3],\n",
    "                hue=y_train[0])\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.xlabel(\"PC5\")\n",
    "plt.ylabel(\"PC6\")\n",
    "sns.scatterplot(x=X_train_pca @ pca.components_[4],\n",
    "                y=X_train_pca @ pca.components_[5],\n",
    "                hue=y_train[0])\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.xlabel(\"PC7\")\n",
    "plt.ylabel(\"PC8\")\n",
    "sns.scatterplot(x=X_train_pca @ pca.components_[6],\n",
    "                y=X_train_pca @ pca.components_[7],\n",
    "                hue=y_train[0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After looking at the scatter plots above colored by target label, I don't have high hopes that PCA will give me any better clue for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MODEL SELECTION\n",
    "Defining some convenience methods here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(layers:     List[tf.keras.layers.Layer],\n",
    "                optimizer:  tf.keras.optimizers.Optimizer | str,\n",
    "                loss:       tf.keras.losses.Loss | str,\n",
    "                metrics:    List[str]) -> tf.keras.models.Model:\n",
    "    \"\"\"Constructs and compiles a Keras model, then prints summary.\"\"\"\n",
    "\n",
    "    model = Sequential(layers)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(hist1, hist2):\n",
    "    \"\"\"Visualizes training performance over two spaces.\"\"\"\n",
    "    performance_1 = pd.DataFrame(hist1.history)\n",
    "    performance_2 = pd.DataFrame(hist2.history)\n",
    "\n",
    "    plt.figure(figsize=(15, 5), dpi=150)\n",
    "    plt.suptitle('Training Performance')\n",
    "\n",
    "    # feature space\n",
    "    plt.subplot(121)\n",
    "    plt.title(\"History 1\")\n",
    "    plt.plot(performance_1['accuracy'], label='Training')\n",
    "    plt.plot(performance_1['val_accuracy'], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0.5, 1)\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    # pca latent space\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"History 2\")\n",
    "    plt.plot(performance_2['accuracy'], label='Training')\n",
    "    plt.plot(performance_2['val_accuracy'], label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0.5, 1)\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MODEL SELECTION: Trial 1\n",
    "Now that I have the feature space `X_train`/`X_val` and the PCA latent space `X_train_pca`/`X_val_pca`, I will predict over both spaces with a low-depth neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "layers_1 = [Dense(100, 'relu', input_shape=(X_train.shape[1],)),\n",
    "            Dense(200, 'relu'),\n",
    "            Dense(100, 'relu'),\n",
    "            Dropout(0.25),\n",
    "\n",
    "            Dense(200, 'relu'),\n",
    "            Dense(300, 'relu'),\n",
    "            Dense(200, 'relu'),\n",
    "            Dense(100, 'relu'),\n",
    "            Dropout(0.25),\n",
    "\n",
    "            Dense(50, 'relu'),\n",
    "            Dense(25, 'relu'),\n",
    "            Dense(1, 'sigmoid')]\n",
    "\n",
    "opt_1 = Adam(learning_rate=0.0005,\n",
    "             beta_1=0.99,\n",
    "             beta_2=0.99)\n",
    "\n",
    "model_1 = build_model(layers_1, opt_1, 'mse', ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fit model to feature space\n",
    "history_1 = model_1.fit(X_train, y_train,\n",
    "                        epochs=100, verbose=1,\n",
    "                        validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fit model to PCA latent space\n",
    "history_pca_1 = model_1.fit(X_train_pca, y_train,\n",
    "                            epochs=100, verbose=1,\n",
    "                            validation_data=(X_val_pca, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize training performance over both spaces\n",
    "plot_history(history_1, history_pca_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MODEL SELECTION: Trial 2\n",
    "Clearly, the model overfit to the data. I wonder if I went a little heavy on the hidden layers and hidden neurons. I'll simplify the model and test performance again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define simpler model\n",
    "layers_2 = [Dense(64, 'relu', input_shape=(X_train.shape[1],)),\n",
    "            Dense(128, 'relu'),\n",
    "            Dense(64, 'relu'),\n",
    "            Dense(1, 'sigmoid')]\n",
    "\n",
    "opt_2 = Adam(learning_rate=0.0008,\n",
    "             beta_1=0.98,\n",
    "             beta_2=0.98)\n",
    "\n",
    "model_2 = build_model(layers_2, opt_2, 'mse', ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fit model to feature space\n",
    "history_2 = model_2.fit(X_train, y_train,\n",
    "                        epochs=100, verbose=1,\n",
    "                        validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fit model to PCA latent space\n",
    "history_pca_2 = model_2.fit(X_train_pca, y_train,\n",
    "                            epochs=100, verbose=1,\n",
    "                            validation_data=(X_val_pca, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize training performance over both spaces\n",
    "plot_history(history_2, history_pca_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# FEATURE ENGINEERING: Trial 3\n",
    "Even with a simpler model, it still heavily overfit to the data. Also, in both trials, the neural network performance about the same on the feature space and the latent space. This is possibly because I'm not dropping any principal components, so the PCA space embedding and the normal feature embedding can be represented as linear transformations of each other, which the neural network may have learned.\n",
    "<br>\n",
    "With these observations in mind, I will forgo the effort for principal component decomposition. Instead, I'll focus on encoding categorical features more efficiently so that their information can be extracted better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up the features\n",
    "\n",
    "# `Name` and `PassengerId` are unique identifiers, probably not highly correlated to the target\n",
    "X_clean = X.drop(['Name', 'PassengerId'], axis=1)\n",
    "\n",
    "# Only dummy encode features with fewer than 5 categories\n",
    "expand_cols = ['HomePlanet', 'Destination']\n",
    "onehot_cols = pd.get_dummies(X_clean[expand_cols])\n",
    "X_clean = X_clean.drop(expand_cols, axis=1)\n",
    "X_clean = X_clean.join(onehot_cols)\n",
    "\n",
    "# Binarize boolean features\n",
    "for col in ('CryoSleep', 'VIP'):\n",
    "    column = X_clean[col].copy()\n",
    "    column[column.isna()] = 0\n",
    "    column = np.zeros(column.shape) + column.to_numpy()\n",
    "    column = pd.Series(column, dtype='uint8')\n",
    "    X_clean[col] = column\n",
    "\n",
    "# Split `Cabin` encoding by splitting it into 3 features\n",
    "cabins = {'Cabin1': [],\n",
    "          'Cabin2': [],\n",
    "          'Cabin3': []}\n",
    "\n",
    "for i, row in X_clean.copy().iterrows():\n",
    "    if type(row['Cabin']) == str:\n",
    "        c1, c2, c3 = row['Cabin'].split('/')\n",
    "\n",
    "        # Numeralize cabin location by summing ASCII values\n",
    "        cabins['Cabin1'].append(sum(ord(c) for c in str(c1)))\n",
    "        cabins['Cabin2'].append(sum(ord(c) for c in str(c2)))\n",
    "        cabins['Cabin3'].append(sum(ord(c) for c in str(c3)))\n",
    "\n",
    "# Semanticize cabin location by min-max scaling the ASCII value sums\n",
    "for i, cabin in cabins.items():\n",
    "    cabins[i] = np.array(cabin)\n",
    "    cabins[i] = (cabins[i] - cabins[i].min()) / (cabins[i].max() / cabins[i].min())\n",
    "\n",
    "X_clean = X_clean.drop('Cabin', axis=1)\n",
    "X_clean = X_clean.join(pd.DataFrame(cabins))\n",
    "\n",
    "X_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize features between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_clean)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_clean.columns)\n",
    "X_scaled[X_scaled.isna()] = 0\n",
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split 20% of data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_clean, test_size=0.2, shuffle=True, random_state=0)\n",
    "\n",
    "print(f\"{X_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{X_val.shape=}\")\n",
    "print(f\"{y_val.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Show feature correlation heatmap after preprocessing\n",
    "plt.figure(figsize=(15, 5), dpi=150)\n",
    "sns.heatmap(X_train.join(y_train).corr(), annot=True, cmap='cubehelix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that the data is preprocessed, every feature shows up on the heatmap. Subsequently, we can see that `CryoSleep` seems to have a moderate positive correlation with the target, while `RoomService`, `Spa`, and `VRDeck` have a mild negative correlation with the target. Also of note, splitting `Cabin` into three features didn't seem to add to the feature's target correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# FEATURE ENGINEERING: Trial 4\n",
    "\n",
    " I will re-engineer the feature set, taking into account the observations made on the heatmap above. I want to try to accentuate the correlations somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up the features\n",
    "X_clean = X.drop(['Name', 'PassengerId', 'Cabin'], axis=1)\n",
    "\n",
    "# Only dummy encode features with fewer than 5 categories\n",
    "expand_cols = ['HomePlanet', 'Destination']\n",
    "onehot_cols = pd.get_dummies(X_clean[expand_cols])\n",
    "X_clean = X_clean.drop(expand_cols, axis=1)\n",
    "X_clean = X_clean.join(onehot_cols)\n",
    "\n",
    "# Binarize boolean features\n",
    "for col in ('CryoSleep', 'VIP'):\n",
    "    column = X_clean[col].copy()\n",
    "    column[column.isna()] = 0\n",
    "    column = np.zeros(column.shape) + column.to_numpy()\n",
    "    column = pd.Series(column)\n",
    "    X_clean[col] = column\n",
    "\n",
    "X_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize features between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_clean)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_clean.columns)\n",
    "X_scaled[X_scaled.isna()] = 0\n",
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Accentuate correlations through logistic expansion (experimental)\n",
    "def logitistic_expansion(a: np.ndarray, magic: int) -> np.ndarray:\n",
    "    \"\"\"I'm making this up, but here's the objective:\n",
    "\n",
    "    1. Take a range of data that is normalized between 0 and 1\n",
    "    2. Accentuate the spread by making lower values lower and higher values higher.\n",
    "    3. See if accentuating the spread improves correlation and model performance.\n",
    "\n",
    "    Algorithm:\n",
    "    - Subtract 0.5 from each value\n",
    "        - Small values will have negative differences\n",
    "        - Large values will have positive differences\n",
    "    - Exponentiate the differences\n",
    "        - Positive differences will create product > 1\n",
    "        - Negative differences will create product < 1\n",
    "    - Scale the value by the exponentiated difference\n",
    "        - Smaller values will get smaller\n",
    "        - Larger values will get larger\n",
    "\n",
    "    Magic is the number of times the accentuation will occur.\n",
    "    \"\"\"\n",
    "    return a * (np.e ** (a - 0.5)) ** magic\n",
    "\n",
    "X_expanded = logitistic_expansion(X_scaled, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split 20% of data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_clean, test_size=0.2, shuffle=True, random_state=0)\n",
    "X_train_exp, X_val_exp, _, _ = train_test_split(X_expanded, y_clean, test_size=0.2, shuffle=True, random_state=0)\n",
    "\n",
    "print(f\"{X_train.shape=}\")\n",
    "print(f\"{X_train_exp.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{X_val.shape=}\")\n",
    "print(f\"{X_val_exp.shape=}\")\n",
    "print(f\"{y_val.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Show feature correlation heatmap after preprocessing\n",
    "plt.figure(figsize=(15, 5), dpi=150)\n",
    "sns.heatmap(X_train_exp.join(y_train).corr(), annot=True, cmap='cubehelix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MODEL SELECTION: Trial 3\n",
    "I can see from the heatmap that the expansion did not work as intended. I predict that the model will fail miserably on the expanded data, but we will try anyway, for science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define another model\n",
    "layers_3 = [Dense(64, 'relu', input_shape=(X_train.shape[1],)),\n",
    "            Dense(128, 'relu'),\n",
    "            Dense(64, 'relu'),\n",
    "            Dense(1, 'sigmoid')]\n",
    "\n",
    "opt_3 = Adam(learning_rate=0.0008,\n",
    "             beta_1=0.98,\n",
    "             beta_2=0.98)\n",
    "\n",
    "model_3 = build_model(layers_3, opt_3, 'mse', ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fit model to feature space\n",
    "history_3 = model_3.fit(X_train, y_train,\n",
    "                        epochs=100, verbose=1,\n",
    "                        validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fit model to feature space\n",
    "history_3_exp = model_3.fit(X_train_exp, y_train,\n",
    "                            epochs=100, verbose=1,\n",
    "                            validation_data=(X_val_exp, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize training performance\n",
    "plot_history(history_3, history_3_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As predicted, model performance on the expanded data did very poorly. However, it is interesting to note that after dropping the `Cabin` feature entirely, the performance of the model on the data after only scaling is improved from before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# FEATURE ENGINEERING: Trial 5\n",
    "Seeing how dropping the `Cabin`, which had close-to-zero correlation, improved model performance, I will drop all of the other \"neutral\" features, hoping that it will reduce noise in the model's training and improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up the features\n",
    "X_clean = X.drop(['Name', 'PassengerId', 'Cabin', 'Age', 'VIP', 'FoodCourt', 'ShoppingMall'], axis=1)\n",
    "\n",
    "# Only dummy encode features with small (<5) expansion\n",
    "expand_cols = ['HomePlanet', 'Destination']\n",
    "onehot_cols = pd.get_dummies(X_clean[expand_cols])\n",
    "X_clean = X_clean.drop(expand_cols, axis=1)\n",
    "X_clean = X_clean.join(onehot_cols)\n",
    "\n",
    "# Binarize `CryoSleep`\n",
    "cryo = X_clean['CryoSleep'].copy()\n",
    "cryo[cryo.isna()] = 0\n",
    "cryo = np.zeros(cryo.shape) + cryo.to_numpy()\n",
    "cryo = pd.Series(cryo)\n",
    "X_clean['CryoSleep'] = cryo\n",
    "\n",
    "X_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize features between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_clean)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_clean.columns)\n",
    "X_scaled[X_scaled.isna()] = 0\n",
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split 20% of data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_clean, test_size=0.2, shuffle=True, random_state=0)\n",
    "\n",
    "print(f\"{X_train.shape=}\")\n",
    "print(f\"{y_train.shape=}\")\n",
    "print(f\"{X_val.shape=}\")\n",
    "print(f\"{y_val.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Show feature correlation heatmap after preprocessing\n",
    "plt.figure(figsize=(15, 5), dpi=150)\n",
    "sns.heatmap(X_train.join(y_train).corr(), annot=True, cmap='cubehelix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MODEL SELECTION: Trial 4\n",
    "Now that only features with a non-neutral correlation factor are in the feature set, I want to see how performance is affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define another model\n",
    "layers_4 = [Dense(64, 'relu', input_shape=(X_train.shape[1],)),\n",
    "            Dense(128, 'relu'),\n",
    "            Dense(64, 'relu'),\n",
    "            Dense(1, 'sigmoid')]\n",
    "\n",
    "opt_4 = Adam(learning_rate=0.0008,\n",
    "             beta_1=0.98,\n",
    "             beta_2=0.98)\n",
    "\n",
    "model_4 = build_model(layers_4, opt_4, 'mse', ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fit model to feature space\n",
    "history_4 = model_4.fit(X_train, y_train,\n",
    "                        epochs=100, verbose=1,\n",
    "                        validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize training performance\n",
    "performance = pd.DataFrame(history_4.history)\n",
    "\n",
    "plt.figure(figsize=(15, 5), dpi=150)\n",
    "plt.plot(performance['accuracy'], label='Training')\n",
    "plt.plot(performance['val_accuracy'], label='Validation')\n",
    "\n",
    "plt.title(\"Training Performance\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.5, 1)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The model performance did improve, but it seems to have leveled out just under 80% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MODEL SELECTION: Trial 5\n",
    "I will try to tune the model architecture and the optimizer to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define another model\n",
    "layers_5 = [Dense(64, 'relu', input_shape=(X_train.shape[1],)),\n",
    "            Dense(128, 'relu'),\n",
    "            Dropout(0.1),\n",
    "\n",
    "            Dense(128, 'relu'),\n",
    "            Dense(128, 'relu'),\n",
    "            Dropout(0.1),\n",
    "\n",
    "            Dense(128, 'relu'),\n",
    "            Dense(128, 'relu'),\n",
    "            Dropout(0.1),\n",
    "\n",
    "            Dense(64, 'relu'),\n",
    "            Dense(1, 'sigmoid')]\n",
    "\n",
    "model_5 = build_model(layers_5, 'adam', 'mse', ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fit model to feature space\n",
    "history_5 = model_5.fit(X_train, y_train,\n",
    "                        epochs=100, verbose=1,\n",
    "                        validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize training performance\n",
    "performance = pd.DataFrame(history_5.history)\n",
    "\n",
    "plt.figure(figsize=(15, 5), dpi=150)\n",
    "plt.plot(performance['accuracy'], label='Training')\n",
    "plt.plot(performance['val_accuracy'], label='Validation')\n",
    "\n",
    "plt.title(\"Training Performance\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.5, 1)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CONCLUSION\n",
    "\n",
    "Through this analysis, I've learned a few key things:\n",
    "- PCA doesn't magically make any dataset linearly separable.\n",
    "- Having way too many features will cause the model to overfit quickly.\n",
    "- Accentuating separation of scaled values actually compresses correlation.\n",
    "- Removing features with close-to-zero target correlation can reduce noise and improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SUBMISSION\n",
    "Now that I've reached the end of the analysis and model selection, I will now predict on the test data and submit to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "X_test = pd.read_csv('./data/test.csv')\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up the features\n",
    "X_test_clean = X_test.drop(['Name', 'PassengerId', 'Cabin', 'Age', 'VIP', 'FoodCourt', 'ShoppingMall'], axis=1)\n",
    "\n",
    "# Only dummy encode features with fewer than 5 categories\n",
    "expand_cols = ['HomePlanet', 'Destination']\n",
    "onehot_cols = pd.get_dummies(X_test_clean[expand_cols])\n",
    "X_test_clean = X_test_clean.drop(expand_cols, axis=1)\n",
    "X_test_clean = X_test_clean.join(onehot_cols)\n",
    "\n",
    "# Binarize `CryoSleep`\n",
    "cryo = X_test_clean['CryoSleep'].copy()\n",
    "cryo[cryo.isna()] = 0\n",
    "cryo = np.zeros(cryo.shape) + cryo.to_numpy()\n",
    "cryo = pd.Series(cryo)\n",
    "X_test_clean['CryoSleep'] = cryo\n",
    "\n",
    "X_test_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize features between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_test_clean)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test_clean.columns)\n",
    "X_test_scaled[X_test_scaled.isna()] = 0\n",
    "X_test_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prediction = model_5.predict(X_test_scaled)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(X_test['PassengerId']).join(pd.DataFrame(prediction))\n",
    "submission.columns = ['PassengerId', 'Transported']\n",
    "submission['Transported'] = submission['Transported'].apply(lambda x: True if x >= 0.5 else False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# FINAL RESULT\n",
    "I placed 1366/2057 with an accuracy score of 0.78372!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
